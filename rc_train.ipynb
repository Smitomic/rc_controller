{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced5eb87-b43c-47aa-8c4d-923c61e95308",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b06157-6c16-4dfc-a2de-d3402f7cecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tomas Tomcany\\PycharmProjects\\rc_controller\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ae377-d88c-4540-963b-4279c4ef3117",
   "metadata": {},
   "source": [
    "# Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf25ee78-29dd-4495-9544-2c86e110eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_classes = [\"accelerate\", \"reverse\", \"leftTurn\", \"rightTurn\", \"leftInPlaceTurn\", \"rightInPlaceTurn\", \"stop\"]\n",
    "\n",
    "directory_path = r\"C:\\Users\\Tomas Tomcany\\PycharmProjects\\rc_controller\\captured_images\"\n",
    "\n",
    "image_paths = []\n",
    "directions = []\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "         # The last part \"zzz\" of the filename xxx_yyy_zzz is the direction control\n",
    "        direction = filename.split('_')[-1] \n",
    "        \n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Append the data to the lists\n",
    "        image_paths.append(image_path)\n",
    "        directions.append(direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15cca3-3fa7-4448-b5fc-c0bf1f037907",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa4c65e-5b8a-4675-95e8-7e4d4f23fb01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m encoded_directions \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(directions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# For neural networks, it is also recommended to apply one hot encoding\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m onehot_encoded_directions \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_directions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\rc_controller\\venv\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:71\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     69\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m num_classes:\n\u001b[1;32m---> 71\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\PycharmProjects\\rc_controller\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\rc_controller\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert categorical values into integers\n",
    "encoded_directions = label_encoder.fit_transform(directions)\n",
    "\n",
    "# For neural networks, it is also recommended to apply one hot encoding\n",
    "onehot_encoded_directions = to_categorical(encoded_directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190bc2b-799a-4650-9486-e58e845fcc30",
   "metadata": {},
   "source": [
    "# Train/Test split\n",
    "80/20 split is generally a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a89e054-c449-4693-887f-234ebd90b148",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split( \u001b[43mimage_paths\u001b[49m, directions, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_valid)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, directions, test_size=0.2)\n",
    "print(f\"Training data: {len(X_train)}\\nValidation data: {len(X_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731db7ab-26a6-473f-be80-210311376c88",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "For artificially increasing the amount of data for training and testing. This can prevent overfitting.\\\n",
    "Geometric transformation – flipping, zooming, cropping, panning...\\\r\n",
    "Color space transformatinos  –change RGB channelge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714900ef-2b4a-47c2-b3a8-65fff7c32ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image):\n",
    "    # Specify the zoom factor\n",
    "    zoom_factor = 1.5\n",
    "    \n",
    "    # Get the image dimensions\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Calculate the new dimensions after zooming\n",
    "    new_height = int(height * zoom_factor)\n",
    "    new_width = int(width * zoom_factor)\n",
    "    \n",
    "    # Resize the image and return\n",
    "    return cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "def flip(image, direction):\n",
    "    # Flip the image\n",
    "    image = cv2.flip(image,1)\n",
    "\n",
    "    # If car is making a turn flip the direction label as well\n",
    "    if direction == \"leftTurn\":\n",
    "        direction == \"rightTurn\"\n",
    "    elif direction == \"rightTurn\":\n",
    "        direction == \"leftTurn\"\n",
    "    elif direction == \"leftinPlaceTurn\":\n",
    "        direction == \"rightInPlaceTurn\"\n",
    "    elif direction == \"rightInPlaceTurn\":\n",
    "        direction == \"leftInPlaceTurn\"\n",
    "   \n",
    "    return image, steering_angle\n",
    "\n",
    "def adjust_brightness(image, max_delta=30):\n",
    "    # Randomly adjust the brightness of the image\n",
    "    delta = np.random.uniform(-max_delta, max_delta)\n",
    "    image = cv2.add(image, np.array([delta]))\n",
    "    return image\n",
    "\n",
    "def translate_shift(image, max_translation=20):\n",
    "    # Randomly translate (shift) the image horizontally and/or vertically\n",
    "    rows, cols, _ = image.shape\n",
    "    dx = np.random.uniform(-max_translation, max_translation)\n",
    "    dy = np.random.uniform(-max_translation, max_translation)\n",
    "    translation_matrix = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    image = cv2.warpAffine(image, translation_matrix, (cols, rows))\n",
    "    return image\n",
    "\n",
    "def perform_random_augmentation(image, direction):\n",
    "    # Randomly pick and apply augmentation to the image\n",
    "    augmentations = [\n",
    "        zoom, flip, adjust_brightness, translate_shift\n",
    "    ]\n",
    "\n",
    "    selected_augmentations = np.random.choice(augmentations, np.random.randint(1, len(augmentations)), replace=False)\n",
    "\n",
    "    for augmentation in selected_augmentations:\n",
    "        if augmentation == flip:\n",
    "            image, direction = augmentation(image, direction)\n",
    "        else:\n",
    "            image = augmentation(image)\n",
    "\n",
    "    return image, direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91590dee-9f9d-471e-af0d-07ffafaa87ff",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed7e27-72d5-4786-9a09-b163516a38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_paths, directions, target_size=(224, 224)):\n",
    "    target_size = (224, 224)\n",
    "    images = []\n",
    "    augmented_directions = []\n",
    "    for path, direction in zip(image_paths, directions)\n",
    "        # Load image using cv2\n",
    "        image = cv2.imread(path)\n",
    "        \n",
    "        # Resize the image to the target size\n",
    "        image = cv2.resize(image, target_size)\n",
    "        \n",
    "        # Perform random augmentation\n",
    "        image, augmented_direction = perform_random_augmentation(image, direction)\n",
    "\n",
    "        # Normalize image\n",
    "        image = cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        # Append the processed image and augmented classifier to the list\n",
    "        images.append(image)\n",
    "        augmented_directions.append(augmented_direction)\n",
    "    \n",
    "    return np.array(images), np.array(augmented_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d7f76-aa50-4d73-b3b1-bfd264b70f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "X_train_processed, y_train_processed = preprocess_images(X_train, y_train)\n",
    "X_valid_processed, y_valid_processed = preprocess_images(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f78345-4a5f-4cd5-94c6-043ccd1f14a7",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14234457-fd30-4972-88e5-c335d9893d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(directions)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    # Dropout layer to reduce overfitting\n",
    "    layers.Dropout(0.2),\n",
    "    # Fully connected layer\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced95f8c-02e1-4b16-a03d-fef042e6e9e0",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335ff27-2ba9-47c1-b5bd-7c3e7c302a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1601360-a29d-49f2-8b6d-2ab1b9dba3b2",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826979-18f1-48e9-8baf-95ae16113608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29206ef2-398c-4af6-b0cb-ff843e650806",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2394c7-fd8d-4b21-9689-70832f70dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_processed, y_train_processed, validation_data=(X_valid_processed, y_valid_processed), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3381ecf-5bff-480e-89df-84cf261c4c27",
   "metadata": {},
   "source": [
    "# Plot accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2567c8f-ef30-4e6d-b335-2f592dc75e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28aa43-095d-40fb-a840-48862163900a",
   "metadata": {},
   "source": [
    "# Plot loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c5aba-412c-4992-9d1c-b4bdb8258776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdadfc-ae57-4838-b693-6082bd949bdb",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e0db0-6675-4b8a-bc24-2a70104021e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_pred_valid = model.predict(X_valid_processed)\n",
    "y_pred_valid_binary = (y_pred_valid > 0.5).astype(int)\n",
    "\n",
    "# Calculate and print classification metrics\n",
    "accuracy_valid = accuracy_score(y_valid_processed, y_pred_valid_binary)\n",
    "precision_valid = precision_score(y_valid_processed, y_pred_valid_binary)\n",
    "recall_valid = recall_score(y_valid_processed, y_pred_valid_binary)\n",
    "f1_valid = f1_score(y_valid_processed, y_pred_valid_binary)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_valid}\")\n",
    "print(f\"Validation Precision: {precision_valid}\")\n",
    "print(f\"Validation Recall: {recall_valid}\")\n",
    "print(f\"Validation F1 Score: {f1_valid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
