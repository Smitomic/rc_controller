{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced5eb87-b43c-47aa-8c4d-923c61e95308",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b06157-6c16-4dfc-a2de-d3402f7cecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tomas Tomcany\\PycharmProjects\\rc_controller\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ae377-d88c-4540-963b-4279c4ef3117",
   "metadata": {},
   "source": [
    "# Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf25ee78-29dd-4495-9544-2c86e110eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_classes = [\"accelerate\", \"reverse\", \"leftTurn\", \"rightTurn\", \"leftInPlaceTurn\", \"rightInPlaceTurn\", \"stop\"]\n",
    "\n",
    "directory_path = r\"C:\\Users\\Tomas Tomcany\\PycharmProjects\\rc_controller\\captured_images\"\n",
    "\n",
    "image_paths = []\n",
    "directions = []\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "         # The last part \"zzz\" of the filename xxx_yyy_zzz is the direction control\n",
    "        direction = filename.split('_')[-1] \n",
    "        \n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Append the data to the lists\n",
    "        image_paths.append(image_path)\n",
    "        directions.append(direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190bc2b-799a-4650-9486-e58e845fcc30",
   "metadata": {},
   "source": [
    "# Train/Test split\n",
    "80/20 split is generally a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a89e054-c449-4693-887f-234ebd90b148",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split( \u001b[43mimage_paths\u001b[49m, directions, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_valid)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, directions, test_size=0.2)\n",
    "print(f\"Training data: {len(X_train)}\\nValidation data: {len(X_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731db7ab-26a6-473f-be80-210311376c88",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "For artificially increasing the amount of data for training and testing. This can prevent overfitting.\\\n",
    "Geometric transformation – flipping, zooming, cropping, panning...\\\r\n",
    "Color space transformatios –change RGB channels\\r\r\n",
    "Kernel filtsrs blurring or sharpening an imageeage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714900ef-2b4a-47c2-b3a8-65fff7c32ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(image):\n",
    "    # Specify the zoom factor\n",
    "    zoom_factor = 1.5\n",
    "    \n",
    "    # Get the image dimensions\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Calculate the new dimensions after zooming\n",
    "    new_height = int(height * zoom_factor)\n",
    "    new_width = int(width * zoom_factor)\n",
    "    \n",
    "    # Resize the image and return\n",
    "    return cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "def flip(image, direction):\n",
    "    # Flip the image\n",
    "    image = cv2.flip(image,1)\n",
    "\n",
    "    # If car is making a turn flip the direction label as well\n",
    "    if direction == \"leftTurn\":\n",
    "        direction == \"rightTurn\"\n",
    "    elif direction == \"rightTurn\":\n",
    "        direction == \"leftTurn\"\n",
    "    elif direction == \"leftinPlaceTurn\":\n",
    "        direction == \"rightInPlaceTurn\"\n",
    "    elif direction == \"rightInPlaceTurn\":\n",
    "        direction == \"leftInPlaceTurn\"\n",
    "   \n",
    "    return image, steering_angle\n",
    "\n",
    "def adjust_brightness(image, max_delta=30):\n",
    "    # Randomly adjust the brightness of the image\n",
    "    delta = np.random.uniform(-max_delta, max_delta)\n",
    "    image = cv2.add(image, np.array([delta]))\n",
    "    return image\n",
    "\n",
    "def translate_shift(image, max_translation=20):\n",
    "    # Randomly translate (shift) the image horizontally and/or vertically\n",
    "    rows, cols, _ = image.shape\n",
    "    dx = np.random.uniform(-max_translation, max_translation)\n",
    "    dy = np.random.uniform(-max_translation, max_translation)\n",
    "    translation_matrix = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    image = cv2.warpAffine(image, translation_matrix, (cols, rows))\n",
    "    return image\n",
    "\n",
    "def perform_random_augmentation(image, direction):\n",
    "    # Randomly pick and apply augmentation to the image\n",
    "    augmentations = [\n",
    "        zoom, flip, adjust_brightness, translate_shift\n",
    "    ]\n",
    "\n",
    "    selected_augmentations = np.random.choice(augmentations, np.random.randint(1, len(augmentations)), replace=False)\n",
    "\n",
    "    for augmentation in selected_augmentations:\n",
    "        if augmentation == flip:\n",
    "            image, direction = augmentation(image, direction)\n",
    "        else:\n",
    "            image = augmentation(image)\n",
    "\n",
    "    return image, direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91590dee-9f9d-471e-af0d-07ffafaa87ff",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed7e27-72d5-4786-9a09-b163516a38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_paths, directions, target_size=(224, 224)):\n",
    "    target_size = (224, 224)\n",
    "    images = []\n",
    "    augmented_directions = []\n",
    "    for path, direction in zip(image_paths, directions)\n",
    "        # Load image using cv2\n",
    "        image = cv2.imread(path)\n",
    "        \n",
    "        # Resize the image to the target size\n",
    "        image = cv2.resize(image, target_size)\n",
    "        \n",
    "        # Perform random augmentation\n",
    "        image, augmented_direction = perform_random_augmentation(image, direction)\n",
    "\n",
    "        # Normalize image\n",
    "        image = cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        \n",
    "        # Append the processed image and augmented classifier to the list\n",
    "        images.append(image)\n",
    "        augmented_directions.append(augmented_direction)\n",
    "    \n",
    "    return np.array(images), np.array(augmented_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d7f76-aa50-4d73-b3b1-bfd264b70f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "X_train_processed, y_train_processed = preprocess_images(X_train, y_train)\n",
    "X_valid_processed, y_valid_processed = preprocess_images(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f78345-4a5f-4cd5-94c6-043ccd1f14a7",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14234457-fd30-4972-88e5-c335d9893d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(directions)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    # Dropout layer to reduce overfitting\n",
    "    layers.Dropout(0.2),\n",
    "    # Fully connected layer\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced95f8c-02e1-4b16-a03d-fef042e6e9e0",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335ff27-2ba9-47c1-b5bd-7c3e7c302a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1601360-a29d-49f2-8b6d-2ab1b9dba3b2",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c826979-18f1-48e9-8baf-95ae16113608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29206ef2-398c-4af6-b0cb-ff843e650806",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2394c7-fd8d-4b21-9689-70832f70dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_processed, y_train_processed validation_data=(X_valid_processed, y_valid_processed), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb70fb-6a00-4fae-abed-526ce3adbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28aa43-095d-40fb-a840-48862163900a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c5aba-412c-4992-9d1c-b4bdb8258776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
